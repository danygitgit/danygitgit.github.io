<!DOCTYPE html>
<html lang="zh-cmn-Hans" prefix="og: http://ogp.me/ns#" class="han-init">
<head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <title>新浪微博爬虫 &mdash; 毛毛虫_Wendy</title>
    <link rel="stylesheet" href="http://localhost:4000/assets/vendor/primer-css/css/primer.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/vendor/primer-markdown/dist/user-content.min.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/vendor/octicons/octicons/octicons.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/components/collection.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/components/repo-card.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/sections/repo-list.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/sections/mini-repo-list.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/components/boxed-group.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/globals/common.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/vendor/share.js/dist/css/share.min.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/globals/responsive.css">
    <link rel="stylesheet" href="http://localhost:4000/assets/css/posts/index.css">
    <!-- Latest compiled and minified CSS -->
    

    
    <link rel="canonical" href="http://localhost:4000/2018/01/30/weibospider/">
    <link rel="alternate" type="application/atom+xml" title="毛毛虫_Wendy" href="http://localhost:4000/feed.xml">
    <link rel="shortcut icon" href="http://localhost:4000/favicon.ico">
    
    <meta property="og:title" content="新浪微博爬虫">
      
    <meta name="keywords" content="sina、weibo、爬虫">
    <meta name="og:keywords" content="sina、weibo、爬虫">
      
    <meta name="description" content="详解新浪微博爬取过程">
    <meta name="og:description" content="详解新浪微博爬取过程">
      
    
    
        
    
    <meta property="og:url" content="http://localhost:4000/2018/01/30/weibospider/">
    <meta property="og:site_name" content="毛毛虫_Wendy">
    <meta property="og:type" content="article">
    <meta property="og:locale" content="zh_CN" />
    
    <meta property="article:published_time" content="2018-01-30">
    
    <script src="http://localhost:4000/assets/vendor/jquery/dist/jquery.min.js"></script>
    <script src="http://localhost:4000/assets/js/jquery-ui.js"></script>
    <script type="text/javascript">
    function toggleMenu() {
        var nav = document.getElementsByClassName("site-header-nav")[0];
        if (nav.style.display == "inline-flex") {
          nav.style.display = "none";
        } else {
          nav.style.display = "inline-flex";
        }
    }
    </script>
</head>
<body class="" data-mz="">
    <header class="site-header">
        <div class="container">
            <h1><a href="http://localhost:4000/" title="毛毛虫_Wendy"><span class="octicon octicon-mark-github"></span> 毛毛虫_Wendy</a></h1>
            <button class="collapsed mobile-visible" type="button" onclick="toggleMenu();">
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
              <span class="icon-bar"></span>
            </button>
            <nav class="site-header-nav" role="navigation">
                
                <a href="http://localhost:4000/" class=" site-header-nav-item" target="" title="Home">Home</a>
                
                <a href="http://localhost:4000/categories/" class=" site-header-nav-item" target="" title="Blog">Blog</a>
                
                <a href="http://localhost:4000/open-source/" class=" site-header-nav-item" target="" title="Open-source">Open-source</a>
                
                <a href="http://localhost:4000/wiki/" class=" site-header-nav-item" target="" title="Wiki">Wiki</a>
                
                <a href="http://localhost:4000/links/" class=" site-header-nav-item" target="" title="Link">Link</a>
                
                <a href="http://localhost:4000/about/" class=" site-header-nav-item" target="" title="About">About</a>
                
            </nav>
        </div>
    </header>
    <!-- / header -->


    <section class="collection-head small geopattern" data-pattern-id="新浪微博爬虫">
<div class="container">
  <div class="columns">
    <div class="column three-fourths">
      <div class="collection-title">
        <h1 class="collection-header">新浪微博爬虫</h1>
        <div class="collection-info">
          
          <span class="meta-info">
            <span class="octicon octicon-calendar"></span> 2018/01/30
          </span>
          
          
          <span class="meta-info">
            <span class="octicon octicon-file-directory"></span>
            <a href="http://localhost:4000/categories/#Spider" title="Spider">Spider</a>
          </span>
          
            <span id="/2018/01/30/weibospider/" class="leancloud_visitors" data-flag-title="新浪微博爬虫">
            <span class="post-meta-divider">|</span>
            <span class="post-meta-item-text"> Hits:  </span>
            <span class="leancloud-visitors-count"></span>
            </span>
          
          
        </div>
      </div>
    </div>
  </div>
</div>
</section>
<!-- / .banner -->
<section class="container content">
<div class="columns">
  <div class="column three-fourths" >
    <article class="article-content markdown-body">
    <p>详解新浪微博爬取过程</p>

<h3 id="前言">前言</h3>

<p>因为科研需要，我从16年8月起就开始跟微博数据打交道，所以从那时开始就不得不想尽办法爬取微博数据，我爬取的内容主要是：博文、发博账号、发文时间、爬取时间、点赞数/评论数/转发数，详情如图1。经过长时间的总结和实验，我完善了切实可行的<a href="https://github.com/DWJWendy/Weibo_Spider">爬虫代码</a>，代码被我放在github上，同样你也可以在我的个人博客open-source里面查看到weibospider项目。</p>

<p>欢迎大家fork和star我的项目,<strong><a href="https://github.com/DWJWendy/Weibo_Spider">项目地址</a></strong>，谢谢！</p>

<p><img src="/images/blog/2018-01-30-1.png" alt="图1" /></p>

<h3 id="环境">环境</h3>

<p>linux+Python3.6+mongo</p>

<p>但是万变不离其中，更改一下便可以用于其他语言和环境。</p>
<h3 id="预登陆">预登陆</h3>

<p>我们都知道微博数据需要先登录才能爬取，而我们解决的办法是使用微博预登陆获得登录需要的必要参数，这一部分在/Prelogin.py 实现的。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def login_weibo(nick , pwd) :
    #==========================获取servertime , pcid , pubkey , rsakv===========================
    # 预登陆请求，获取到若干参数
    prelogin_url = 'http://login.sina.com.cn/sso/prelogin.php?entry=weibo&amp;callback=sinaSSOController.preloginCallBack&amp;su=%s&amp;rsakt=mod&amp;checkpin=1&amp;client=ssologin.js(v1.4.15)&amp;_=1400822309846' % nick
    preLogin = getData(prelogin_url)
    # 下面获取的四个值都是接下来要使用的
    servertime = re.findall('"servertime":(.*?),' , preLogin.decode('utf-8'))[0]
    pubkey = re.findall('"pubkey":"(.*?)",' , preLogin.decode('utf-8'))[0]
    rsakv = re.findall('"rsakv":"(.*?)",' ,preLogin.decode('utf-8'))[0]
    nonce = re.findall('"nonce":"(.*?)",' , preLogin.decode('utf-8'))[0]

    #===============对用户名和密码加密================
    # 好，你已经来到登陆新浪微博最难的一部分了，如果这部分没有大神出来指点一下，那就真是太难了，我也不想多说什么，反正就是各种加密，最后形成了加密后的su和sp
    su = base64.b64encode(bytes(urllib.request.quote(nick) , encoding = 'utf-8'))
    rsaPublickey = int(pubkey , 16)
    key = rsa.PublicKey(rsaPublickey , 65537)
    #稍微说一下的是在我网上搜到的文章中，有些文章里并没有对拼接起来的字符串进行bytes，这是python3的新方法好像是。rsa.encrypt需要一个字节参数，这一点和之前不一样。其实上面的base64.b64encode也一样
    message = bytes(str(servertime) + '\t' + str(nonce) + '\n' + str(pwd) , encoding = 'utf-8')
    sp = binascii.b2a_hex(rsa.encrypt(message , key))
    #=======================登录=======================
    #param就是激动人心的登陆post参数，这个参数用到了若干个上面第一步获取到的数据，可说的不多
    param = {'entry' : 'weibo' , 'gateway' : 1 , 'from' : '' , 'savestate' : 7 , 'useticket' : 1 , 'pagerefer' : 'http://login.sina.com.cn/sso/logout.php?entry=miniblog&amp;r=http%3A%2F%2Fweibo.com%2Flogout.php%3Fbackurl%3D' , 'vsnf' : 1 , 'su' : su , 'service' : 'miniblog' , 'servertime' : servertime , 'nonce' : nonce , 'pwencode' : 'rsa2' , 'rsakv' : rsakv , 'sp' : sp , 'sr' : '1680*1050' ,
             'encoding' : 'UTF-8' , 'prelt' : 961 , 'url' : 'http://weibo.com/ajaxlogin.php?framelogin=1&amp;callback=parent.sinaSSOController.feedBackUrlCallBack'}
    # 这里就是使用postData的唯一一处，也很简单
    s = postData('http://login.sina.com.cn/sso/login.php?client=ssologin.js(v1.4.15)' , param)
    # 好了，当你的代码执行到这里时，已经完成了大部分了，可是有很多爬虫童鞋跟我一样还就栽在了这里，假如你跳过这里直接去执行获取粉丝的这几行代码你就会发现你获取的到还是让你登陆的页面，真郁闷啊，我就栽在这里长达一天啊
    # 好了，我们还是继续。这个urll是登陆之后新浪返回的一段脚本中定义的一个进一步登陆的url，之前还都是获取参数和验证之类的，这一步才是真正的登陆，所以你还需要再一次把这个urll获取到并用get登陆即可
    urll = re.findall("location.replace\(\'(.*?)\'\);" , s)[0]
    getData(urll)
    #======================获取粉丝====================
    # 如果你没有跳过刚才那个urll来到这里的话，那么恭喜你！你成功了，接下来就是你在新浪微博里畅爬的时候了，获取到任何你想获取到的数据了！
    # 可以尝试着获取你自己的微博主页看看，你就会发现那是一个多大几百kb的文件了

</code></pre></div></div>

<p><strong>但是需要注意这行代码，有时候会出现匹配不成功的问题，所以这个时候你需要打印出s检查一下问题</strong></p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>urll = re.findall("location.replace\(\'(.*?)\'\);" , s)[0]
</code></pre></div></div>

<h3 id="获取爬取账号的url">获取爬取账号的URL</h3>

<p>这个过程你需要分析账号的规律，因为不同微博账号，可以代码规律不一样，url获取通过查看元素，查找到对应页面的地址。</p>

<p><img src="/images/blog/2018-01-30-2.png" alt="图2" /></p>

<h3 id="解析页码">解析页码</h3>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    def get_content(self,text):
        mongo = MongoDB()
        reg = '<span class="nt">&lt;em&gt;</span>(\d+)<span class="nt">&lt;/em&gt;</span>'
        logger.info('解析获取网页数据...')
        content = json.loads(text.decode("ascii"))['data']
        soup = BeautifulSoup("<span class="nt">&lt;html&gt;&lt;head&gt;&lt;/head&gt;&lt;body&gt;</span>" + content + "<span class="nt">&lt;/body&gt;&lt;/html&gt;</span>", "lxml")
        tmp = soup.find_all("div", attrs={"class": "WB_detail"})
        tmp2 = soup.find_all("div", attrs={"class":"WB_handle"})
        if len(tmp) &gt; 0 :
            for i in range(len(tmp)):
                item = {}
                item["nickname"] = tmp[i].find("div", attrs={"class": "WB_info"}).find("a").get_text()
                item["Post"] = tmp[i].find("div", attrs={"class": "WB_text W_f14"}).get_text().replace("\n", "").replace(" ","").replace( "\u200b", "")
                # -*- 爬取发布时间 -*-
                item["Pubtime"] = tmp[i].find("a", attrs={"class": "S_txt2"}).get("title")
                # -*- 爬取转发数 -*-
                if re.findall(reg,str(tmp2[i].find("span", attrs={"class": "line S_line1","node-type":"forward_btn_text"})), re.S):
                    item["Transfer_num"] = int(re.findall(reg,str(tmp2[i].find("span", attrs={"class": "line S_line1","node-type":"forward_btn_text"})), re.S)[0])
                else:
                    item["Transfer_num"] = 0
                # -*- 爬取评论数 -*-
                if re.findall(reg, str(tmp2[i].find("span", attrs={"class": "line S_line1", "node-type": "comment_btn_text"})), re.S):
                    item["Comment_num"] = int(re.findall(reg, str(tmp2[i].find("span", attrs={"class": "line S_line1", "node-type": "comment_btn_text"})), re.S)[0])
                else:
                    item["Comment_num"] = 0
                # -*- 爬取点赞数 -*-
                if re.findall(reg, str(tmp2[i].find("span", attrs={"node-type": "like_status"})), re.S):
                    item["Like_num"] = int(re.findall(reg, str(tmp2[i].find("span", attrs={"node-type": "like_status"})), re.S)[0])
                else:
                    item["Like_num"] = 0
                item["Scraltime"] = time.strftime("%Y-%m-%d %H:%M:%S", time.localtime())
                if mongo.process_item(item)== "null":
                    break
                else:
                    continue

</code></pre></div></div>

<h3 id="定时登录设置">定时登录设置</h3>

<p>这一部分是我参考别人的代码，你也可以用参考其他方式</p>
<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>import sched, time
from program.Spider import Weibo_Spider
from program.Prelogin import getData
from program.logfile import logger


# 初始化sched模块的scheduler类
# 第一个参数是一个可以返回时间戳的函数，第二个参数可以在定时未到达之前阻塞。
schedule = sched.scheduler(time.time, time.sleep)

def domain():
    weibospider = Weibo_Spider()
    ID_urls = weibospider.ID_urls
    for i in range(len(ID_urls)):
        for j in range(len(ID_urls[i])):
            logger.info('正在爬取第'+str(i)+"个账号 第"+str(j+1)+"条网页")
            weibospider.get_content(text=getData(ID_urls[i][j]))

def perform(inc):
    schedule.enter(inc, 0, perform, (inc,))
    domain()  # 需要周期执行的函数

def mymain():
    schedule.enter(0, 0, perform, (86400,))
</code></pre></div></div>

<h3 id="预览结果">预览结果</h3>

<p>更改微博账号以及你自己url地址和微博账号id，然后运行/main.py文件，爬取过程如下图。</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>if __name__ == "__main__":
    mymain()
    schedule.run()  # 开始运行，直到计划时间队
</code></pre></div></div>

<p><img src="/images/blog/2018-01-30-3.png" alt="图3" /></p>

<h2 id="tips">Tips</h2>

<ul>
  <li>微博账号可以自己去淘宝买，如果爬取的内容比较多最好多买几个换着用。</li>
  <li>前面也说了有时候正则匹配会出现问题，我也不知道为啥会时不时不能用，总的来说用起来没什么毛病。</li>
  <li>个人觉得这些代码可以scrapy框架再整理一下。</li>
  <li>如果大家发现还有什么毛病也欢迎联系我dengwenjun818@gmail.com。</li>
</ul>

    </article>
    <div class="share">
      <div class="share-component"></div>
    </div>
    <div class="comment">
      

  

  
        <div id="gitalk-container"></div>
        <link rel="stylesheet" href="https://unpkg.com/gitalk/dist/gitalk.css">
        <script src="https://unpkg.com/gitalk/dist/gitalk.min.js"></script>
        <script>
        var gitalk = new Gitalk({
            id: '/2018/01/30/weibospider/',
            clientID: 'ef4ee0f3d879e724a46c',
            clientSecret: '2a98b4739a3712bb347ed63cf50b5a07e41f4c48',
            repo: 'blog-comments',
            owner: 'DWJWendy',
            admin: ['DWJWendy'],
            labels: ['gitment'],
            perPage: 50,
        })
        gitalk.render('gitalk-container')
        </script>
  


    </div>
  </div>
  <div class="column one-fourth">
    
<h3>Search</h3>
<div id="site_search">
    <input type="text" id="search_box" placeholder="Search">
</div>

<ul id="search_results"></ul>

<link rel="stylesheet" type="text/css" href="http://localhost:4000/assets/css/modules/sidebar-search.css">
<script src="http://localhost:4000/assets/js/jekyll-search.min.js"></script>
<script src="http://localhost:4000/assets/js/search.js"></script>

<script type="text/javascript">
SimpleJekyllSearch({
    searchInput: document.getElementById('search_box'),
    resultsContainer: document.getElementById('search_results'),
    json: 'http://localhost:4000/assets/search_data.json',
    searchResultTemplate: '<li><a href="{url}" title="{desc}">{title}</a></li>',
    noResultsText: 'No results found',
    limit: 10,
    fuzzy: false,
    exclude: ['Welcome']
})
</script>

    

    
<h3 class="post-directory-title mobile-hidden">Table of Contents</h3>
<div id="post-directory-module" class="mobile-hidden">
  <section class="post-directory">
  <!-- Links that trigger the jumping -->
  <!-- Added by javascript below -->
  <dl></dl>
  </section>
</div>

<script src="http://localhost:4000/assets/js/jquery.toc.js"></script>

  </div>
</div>
</section>
<!-- /section.content -->

    <footer class="container">
        <div class="site-footer" role="contentinfo">
            <div class="copyright left mobile-block">
                    © 2018
                    <span title="Wendy">Wendy</span>
                    <a href="javascript:window.scrollTo(0,0)" class="right mobile-visible">TOP</a>
            </div>

            <ul class="site-footer-links right mobile-hidden">
                <li>
                    <a href="javascript:window.scrollTo(0,0)" >TOP</a>
                </li>
            </ul>
            <a href="http://github.com/DWJWendy/DWJWendy.github.io" target="_blank" aria-label="view source code">
                <span class="mega-octicon octicon-mark-github" title="GitHub"></span>
            </a>
            <ul class="site-footer-links mobile-hidden">
                
                <li>
                    <a href="http://localhost:4000/" title="Home" target="">Home</a>
                </li>
                
                <li>
                    <a href="http://localhost:4000/categories/" title="Blog" target="">Blog</a>
                </li>
                
                <li>
                    <a href="http://localhost:4000/open-source/" title="Open-source" target="">Open-source</a>
                </li>
                
                <li>
                    <a href="http://localhost:4000/wiki/" title="Wiki" target="">Wiki</a>
                </li>
                
                <li>
                    <a href="http://localhost:4000/links/" title="Link" target="">Link</a>
                </li>
                
                <li>
                    <a href="http://localhost:4000/about/" title="About" target="">About</a>
                </li>
                
                <li><a href="http://localhost:4000/feed.xml"><span class="octicon octicon-rss" style="color:orange;"></span></a></li>
            </ul>

        </div>
    </footer>
    <!-- / footer -->
    <script src="http://localhost:4000/assets/vendor/share.js/dist/js/share.min.js"></script>
    <script src="http://localhost:4000/assets/js/geopattern.js"></script>
    <script src="http://localhost:4000/assets/js/prism.js"></script>
    <link rel="stylesheet" href="http://localhost:4000/assets/css/globals/prism.css">
    <script>
      jQuery(document).ready(function($) {
        // geopattern
        $('.geopattern').each(function(){
          $(this).geopattern($(this).data('pattern-id'));
        });
       // hljs.initHighlightingOnLoad();
      });
    </script>

    

    

    

    

    
    <div style="display:none">
      <script>
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');

        ga('create', 'UA-80669434-1', 'auto');
        ga('send', 'pageview');

      </script>
    </div>
    
</body>
</html>



  <script src="https://code.jquery.com/jquery-3.2.0.min.js"></script>
  <script src="https://cdn1.lncld.net/static/js/av-core-mini-0.6.1.js"></script>
  <script>AV.initialize("EfLTUbK2fJ0dYLAq609NQ2k4-gzGzoHsz", "5Agxeshbunhh9y6XQftwEvGj");</script>
  <script>
    function showHitCount(Counter) {
      /* 这是给一个页面中有多篇文章时所调用的，例如博客主界面或者存档界面。
      */
      var query = new AV.Query(Counter);
      var entries = [];
      var $visitors = $(".leancloud_visitors");

      // 获取页面中所有文章的id（page.url）
      $visitors.each(function () {
        entries.push( $(this).attr("id").trim() );
      });

      // 批量查询
      query.containedIn('url', entries);
      query.find()
        .done(function (results) {
          var COUNT_CONTAINER_REF = '.leancloud-visitors-count';

          if (results.length === 0) {
            $visitors.find(COUNT_CONTAINER_REF).text(0);
            return;
          }

          for (var i = 0; i < results.length; i++) {
            var item = results[i];
            var url = item.get('url');
            var hits = item.get('hits');// 获取点击次数
            var element = document.getElementById(url);

            // 显示点击次数
            $(element).find(COUNT_CONTAINER_REF).text(hits);
          }
          for(var i = 0; i < entries.length; i++) {
            var url = entries[i];
            var element = document.getElementById(url);
            var countSpan = $(element).find(COUNT_CONTAINER_REF);
            if( countSpan.text() == '') {
              countSpan.text(0);
            }
          }
        })
        .fail(function (object, error) {
          console.log("Error: " + error.code + " " + error.message);
        });
    }

    function addCount(Counter) {
      // 页面（博客文章）中的信息：leancloud_visitors
      // id为page.url， data-flag-title为page.title
      var $visitors = $(".leancloud_visitors");
      var url = $visitors.attr('id').trim();
      var title = $visitors.attr('data-flag-title').trim();
      var query = new AV.Query(Counter);

      // 只根据文章的url查询LeanCloud服务器中的数据
      query.equalTo("url", url);
      query.find({
        success: function(results) {
          if (results.length > 0) {//说明LeanCloud中已经记录了这篇文章
            var counter = results[0];
            counter.fetchWhenSave(true);
            counter.increment("hits");// 将点击次数加1
            counter.save(null, {
              success: function(counter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(counter.get('hits'));
              },
              error: function(counter, error) {
                console.log('Failed to save Visitor num, with error message: ' + error.message);
              }
            });
          } else {
            // 执行这里，说明LeanCloud中还没有记录此文章
            var newcounter = new Counter();
            /* Set ACL */
            var acl = new AV.ACL();
            acl.setPublicReadAccess(true);
            acl.setPublicWriteAccess(true);
            newcounter.setACL(acl);
            /* End Set ACL */
            newcounter.set("title", title);// 把文章标题
            newcounter.set("url", url); // 文章url
            newcounter.set("hits", 1); // 初始点击次数：1次
            newcounter.save(null, { // 上传到LeanCloud服务器中
              success: function(newcounter) {
                var $element = $(document.getElementById(url));
                $element.find('.leancloud-visitors-count').text(newcounter.get('hits'));
              },
              error: function(newcounter, error) {
                console.log('Failed to create');
              }
            });
          }
        },
        error: function(error) {
          console.log('Error:' + error.code + " " + error.message);
        }
      });
    }

    $(function() {
      var Counter = AV.Object.extend("Counter");
      if ($('.leancloud_visitors').length == 1) {
        // in post.html, so add 1 to hit counts
        addCount(Counter);
      } else if ($('.post-link').length > 1){
        // in index.html, there are many 'leancloud_visitors' and 'post-link', so just show hit counts.
        showHitCount(Counter);
      }
    });
  </script>
